{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fb3703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02450a37",
   "metadata": {},
   "source": [
    "# UTKFace\n",
    "\n",
    "https://github.com/yu4u/age-estimation-pytorch/tree/master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a93a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 7.39 µs\n",
      "Found GPU at: cuda:0\n",
      "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n",
      "root        1027  0.0  0.0  39304 20304 ?        Ss    2021   0:00 /usr/bin/python3 /usr/bin/networkd-dispatcher --run-startup-triggers\n",
      "root        1166  0.0  0.0 118120 22992 ?        Ssl   2021   0:00 /usr/bin/python3 /usr/share/unattended-upgrades/unattended-upgrade-shutdown --wait-for-signal\n",
      "hjchris     2788  0.1  0.1 342440 150164 ?       Sl    2021  11:11 /home/hjchris/research/mitigating/.venv/bin/python /home/hjchris/research/mitigating/.venv/bin/jupyter-notebook --no-browser --port=8888\n",
      "hjchris    16710  0.0  0.0 902444 94368 ?        Ssl   2021   0:31 /home/hjchris/research/mitigating/.venv/bin/python -m ipykernel_launcher -f /home/hjchris/.local/share/jupyter/runtime/kernel-86170f7f-479f-4d41-93c0-1f2fed830e35.json\n",
      "hjchris   311305 15.3  4.1 28226848 5432180 ?    Ssl   2021 603:33 /home/hjchris/research/mitigating/.venv/bin/python -m ipykernel_launcher -f /home/hjchris/.local/share/jupyter/runtime/kernel-0d89be48-8864-4995-977f-2a5be61b6b32.json\n",
      "hjchris   465203  6.1  0.5 15680440 698580 ?     Ssl  10:15   0:12 /home/hjchris/research/mitigating/.venv/bin/python -m ipykernel_launcher -f /home/hjchris/.local/share/jupyter/runtime/kernel-0ad89c5a-66a0-4941-9f2c-f342d19558fc.json\n",
      "hjchris   465297 75.0  0.0   9500  3256 pts/4    Ss+  10:18   0:00 /bin/bash -c ps -aux|grep python\n",
      "hjchris   465299  0.0  0.0   9040   728 pts/4    S+   10:18   0:00 grep python\n",
      "\n",
      "Real Time: ['0102_22']\n",
      "today: 0102_22\n"
     ]
    }
   ],
   "source": [
    "import gc \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np \n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%time\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Found GPU at: {}'.format(device))\n",
    "\n",
    "!/opt/bin/nvidia-smi -L\n",
    "!ps -aux|grep python\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "today = !date +\"%m%d_%y\"\n",
    "print(f\"\\nReal Time: {today}\")\n",
    "today = today[0]\n",
    "print(f\"today: {today}\")\n",
    "\n",
    "\n",
    "sys.path.append(f\"/home/hjchris/repo/utkface-estimation\")\n",
    "sys.path.append(f\"/home/hjchris/repo/pytorch-ssd-master\")\n",
    "from vision.utils.misc import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff7575",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b61f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/hjchris/research/mitigating/chris/utk_ckpt/0102_22\n",
      "File Name: 0102_22-1.log\n",
      "logging filename: /home/hjchris/research/mitigating/chris/utk_ckpt/0102_22/0102_22-1.log\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "file_item=1\n",
    "logging_root = f'/home/hjchris/research/mitigating/chris/utk_ckpt/'\n",
    "logging_folder = logging_root + f'{today}'\n",
    "logging_filename = f\"{today}-{file_item}.log\"\n",
    "\n",
    "if not os.path.isdir(logging_folder):\n",
    "    os.mkdir(logging_folder)\n",
    "    print(f\"Created directory: {logging_folder}\")\n",
    "\n",
    "while os.path.exists(os.path.join(logging_folder, logging_filename)):\n",
    "    file_item += 1\n",
    "    logging_filename = f\"{today}-{file_item}.log\"\n",
    "\n",
    "print(f\"File Name: {logging_filename}\")\n",
    "print(f\"logging filename: {logging_folder}/{logging_filename}\")\n",
    "\n",
    "# basicConfig = logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' )\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "streamHandler = logging.StreamHandler(sys.stdout)\n",
    "streamHandler.setFormatter(formatter)\n",
    "\n",
    "fileHandler=logging.FileHandler(\"{0}/{1}\".format(logging_folder, logging_filename))\n",
    "fileHandler.setFormatter(formatter)\n",
    "\n",
    "train_log = logging.getLogger(\"train\")\n",
    "train_log.setLevel(logging.INFO)\n",
    "if train_log.hasHandlers(): \n",
    "    train_log.removeHandler(fileHandler)\n",
    "    train_log.removeHandler(streamHandler)\n",
    "train_log.addHandler(fileHandler)\n",
    "train_log.addHandler(streamHandler)\n",
    "\n",
    "log_dir = logging_root + 'evaluations'\n",
    "inf_log = logging.getLogger(\"Inference\")\n",
    "inf_log.setLevel(logging.INFO)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975bfac7",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbfb93",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc02a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Main Script for Gender, Age, and Ethnicity identification on the cleaned UTK Dataset.\n",
    "    The dataset can be found here (https://www.kaggle.com/nipunarora8/age-gender-and-ethnicity-face-data-csv)\n",
    "    I implemented a MultiLabelNN that performs a shared high-level feature extraction before creating a \n",
    "    low-level neural network for each classification desired (gender, age, ethnicity).\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "# import argparse\n",
    "# torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# sys.path.append(f\"/home/hjchris/repo/utkface-estimation\")\n",
    "# from CustomUTK import UTKDataset\n",
    "# from MultNN import TridentNN\n",
    "\n",
    "def _xavier_init_(m: nn.Module):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "'''\n",
    "    Add arguments for argparse\n",
    "    Arguments:\n",
    "     - epochs : (int) Number of epochs to train\n",
    "     - lr : (float) Learning rate for the model\n",
    "     - pre-trained : (bool) whether or not to load the pre-trained model\n",
    "'''\n",
    "\n",
    "'''\n",
    "    Function to read in the data\n",
    "    Inputs: None\n",
    "    Outputs:\n",
    "     - train_loader : Custom PyTorch DataLoader for training data from UTK Face Dataset\n",
    "     - test_loader : Custom PyTorch DataLoader for testing UTK Face Dataset\n",
    "     - class_nums : Dictionary that stores the number of unique variables for each class (used in NN)\n",
    "'''\n",
    "\n",
    "class UTKDataset(Dataset):\n",
    "    ''' =================================================================\n",
    "        Inputs:\n",
    "            dataFrame : Pandas dataFrame\n",
    "            transform : The transform to apply to the dataset\n",
    "    ===================================================================== '''\n",
    "    def __init__(self, dataFrame, transform=None):\n",
    "        # read in the transforms\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Use the dataFrame to get the pixel values\n",
    "        data_holder = dataFrame.pixels.apply(lambda x: np.array(x.split(\" \"),dtype=float))\n",
    "        arr = np.stack(data_holder)\n",
    "        arr = arr / 255.0\n",
    "        arr = arr.astype('float32')\n",
    "        arr = arr.reshape(arr.shape[0], 48, 48, 1)\n",
    "        # reshape into 48x48x1\n",
    "        self.data = arr\n",
    "        \n",
    "        # get the age, gender, and ethnicity label arrays\n",
    "        self.age_label = np.array(dataFrame.bins[:])         # Note : Changed dataFrame.age to dataFrame.bins\n",
    "        self.gender_label = np.array(dataFrame.gender[:])\n",
    "        self.eth_label = np.array(dataFrame.ethnicity[:])\n",
    "    \n",
    "    # override the length function\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # override the getitem function\n",
    "    def __getitem__(self, index):\n",
    "        # load the data at index and apply transform\n",
    "        data = self.data[index]\n",
    "        data = self.transform(data)\n",
    "        \n",
    "        # load the labels into a list and convert to tensors\n",
    "        labels = torch.tensor((self.age_label[index], self.gender_label[index], self.eth_label[index]))\n",
    "        \n",
    "        # return data labels\n",
    "        return data, labels\n",
    "\n",
    "def read_data(dataFrame_path, train_batch, attack=False, test_split = 0.2, attack_split = 0.2, log=print):\n",
    "    # Read in the dataframe\n",
    "    dataFrame = pd.read_csv(dataFrame_path, compression='gzip')\n",
    "\n",
    "    # Construct age bins\n",
    "    age_bins = [0,10,15,20,25,30,40,50,60,120]\n",
    "    age_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    dataFrame['bins'] = pd.cut(dataFrame.age, bins=age_bins, labels=age_labels)\n",
    "\n",
    "    # Split into training and testing\n",
    "    train_dataFrame, test_dataFrame = train_test_split(dataFrame, test_size=test_split)\n",
    "    if attack:\n",
    "        train_dataFrame, train_dataFrame_attack = train_test_split(train_dataFrame, test_size = attack_split)\n",
    "        test_dataFrame, test_dataFrame_attack = train_test_split(test_dataFrame, test_size = attack_split)\n",
    "    \n",
    "\n",
    "    # get the number of unique classes for each group\n",
    "    class_nums = {'age_num':len(dataFrame['bins'].unique()), 'ethnicity_num':len(dataFrame['ethnicity'].unique()),\n",
    "                  'gender_num':len(dataFrame['gender'].unique())}\n",
    "\n",
    "    # Define train and test transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomCrop(48),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Normalize((0.49,), (0.23,))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.49,), (0.23,))\n",
    "    ])\n",
    "\n",
    "    # Construct the custom pytorch datasets\n",
    "    train_set = UTKDataset(train_dataFrame, transform=train_transform)\n",
    "    test_set = UTKDataset(test_dataFrame, transform=test_transform)\n",
    "    if attack:\n",
    "        train_set_attack = UTKDataset(train_dataFrame_attack, transform=train_transform)\n",
    "        test_set_attack = UTKDataset(train_dataFrame_attack, transform=train_transform)\n",
    "\n",
    "    # Load the datasets into dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=train_batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "    \n",
    "    if attack:\n",
    "        train_loader_attack = DataLoader(train_set_attack, batch_size=train_batch, shuffle=True)\n",
    "        test_loader_attack = DataLoader(test_set_attack, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Sanity Check\n",
    "    log(f\"Train Loader:{len(train_loader)}(batch size:{train_batch}), Test Laoder:{len(test_loader)}\")\n",
    "    for X, y in train_loader:\n",
    "        log(f'Shape of training X: {X.shape}')\n",
    "        log(f'Shape of y: {y.shape}')\n",
    "        break\n",
    "        \n",
    "    if attack: \n",
    "        log(f\"[Attack] Train Loader:{len(train_loader_attack)}, Test Laoder:{len(test_loader_attack)}\")\n",
    "        for X, y in train_loader_attack:\n",
    "            log(f'Shape of training X: {X.shape}')\n",
    "            log(f'Shape of y: {y.shape}')\n",
    "            break\n",
    "        return train_loader, test_loader, train_loader_attack, test_loader_attack, class_nums\n",
    "    else:\n",
    "        return train_loader, test_loader, class_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315907c8",
   "metadata": {},
   "source": [
    "# MaxEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c070db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxtrain_Network:\n",
    "    def __init__(self, nets, optimizers, alpha = None, sensitive_class = None, public_classes = None, log=print):\n",
    "        \n",
    "        self.log=log\n",
    "        self.net = nets[0]                \n",
    "        self.discriminator = nets[1]\n",
    "        self.public = nets[2]\n",
    "        \n",
    "        self.optimizer = optimizers[0]\n",
    "        self.discriminator_optimizer = optimizers[1]\n",
    "        self.public_optimizer = optimizers[2]\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.sensitive_classes = sensitive_class\n",
    "        self.public_classes = public_classes\n",
    "        \n",
    "        self.entropy_loss = EntropyLoss()\n",
    "        self.softmaxlayer = nn.Softmax(dim=1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.nll_loss = nn.NLLLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.cross_entloss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.images = torch.zeros(0, 0, 0)\n",
    "        self.boxes = torch.zeros(0, 0, 0)\n",
    "        self.labels = torch.zeros(0, dtype=torch.long)\n",
    "        self.sensitive_labels = torch.zeros(0, dtype=torch.long)\n",
    "        self.public_labels = torch.zeros(0, dtype=torch.long)\n",
    "        self.grad_norm = torch.tensor(1.0)\n",
    "\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.images=self.images.cuda()\n",
    "            self.boxes=self.boxes.cuda()\n",
    "            self.labels=self.labels.cuda()\n",
    "            self.grad_norm=self.grad_norm.cuda()\n",
    "\n",
    "            if self.discriminator:\n",
    "                self.sensitive_labels = self.sensitive_labels.cuda()\n",
    "                self.alpha = self.alpha.to(device) \n",
    "            \n",
    "            if self.public:\n",
    "                self.public_labels = self.public_labels.cuda()\n",
    "        \n",
    "        self.meters_reset()\n",
    "\n",
    "    def meters_reset(self):\n",
    "        self.running_loss = 0.0\n",
    "        self.running_regression_loss = 0.0\n",
    "        self.running_classification_loss = 0.0\n",
    "        self.running_regression_loss_dis = 0.0\n",
    "        self.running_discriminator_loss = 0.0\n",
    "        self.report_grad_norm = 0.0\n",
    "        self.running_ssd_loss = 0.0\n",
    "        self.running_discriminator_entropy = 0.0\n",
    "        self.running_public_loss = 0.0\n",
    "        self.running_public_entropy = 0.0\n",
    "\n",
    "    def online_find_labels(self, labels, target_classes=[15]):\n",
    "        label_temp = torch.zeros(len(labels), dtype=torch.long)\n",
    "        for j in range(len(label_temp)):\n",
    "            label_temp[j] = (1 if any(items in labels[j].unique() for items in target_classes) else 0)\n",
    "        return label_temp\n",
    "\n",
    "    def compute_gradnorm(self):\n",
    "        grad_sum = 0\n",
    "        for param in list(filter(lambda p: p.grad is not None, self.net.parameters())):\n",
    "            grad_sum += math.pow(param.grad.data.norm(), 2) \n",
    "        return math.sqrt(grad_sum)\n",
    "    \n",
    "    def call_param(self, net_header, name, print_out=False):\n",
    "        all_param = [param.detach().clone() for param in net_header.parameters()] \n",
    "        if print_out: print(f\"{name}:{all_param[0][0][0]}\")\n",
    "        return all_param\n",
    "\n",
    "    def assert_param(self, p1, p2):\n",
    "        assert len(p1) == len(p2)\n",
    "        for i in range(len(p1)): assert torch.sum(p1[i] == p2[i]) == p1[i].numel()\n",
    "\n",
    "    def grad_summation(self, grads):\n",
    "        grad_sum = 0 \n",
    "        for grad in grads:\n",
    "            if grad != None: grad_sum += torch.sum(grad)\n",
    "        return grad_sum\n",
    "\n",
    "    def train_net(self, net, net_optim, net_input, net_labels):\n",
    "        \n",
    "        net_input = net(net_input)\n",
    "        net_loss = self.cross_entloss(net_input, net_labels)\n",
    "\n",
    "        # Backprop\n",
    "        net_optim.optimizer.zero_grad()\n",
    "        net_loss.backward()\n",
    "        _, grad_norm, _ = net_optim.step_all()\n",
    "\n",
    "        return net_loss, grad_norm\n",
    "\n",
    "    \n",
    "\n",
    "    def arl_train(self, train_loader, private_label, public_label, d_loc=None, privacy_flag=True, \n",
    "                  public_flag = False, arl_setting='maxent-arl', debug_steps=100, epoch=-1, verbose=False, original=True):\n",
    "        \n",
    "        ''' ===================================================================================\n",
    "        Main Algorithm:\n",
    "            1. Train encoder and decoder with discriminator loss\n",
    "            2. Train discriminator\n",
    "        ========================================================================================'''\n",
    "        self.net.train(True)\n",
    "        if privacy_flag: self.discriminator.train(True)\n",
    "        if public_flag: self.public.train(True)\n",
    "        self.meters_reset()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "            # Prepare data\n",
    "            if i%20 == 0 and i: print(end='.')\n",
    "            labels = y[:,0].to(device), y[:,1].to(device), y[:,2].to(device)\n",
    "            X = X.to(device)\n",
    "\n",
    "            # 1. Train Encoder + Predictor =============\n",
    "            z = self.net(X, d_loc)\n",
    "                    \n",
    "            if public_flag:\n",
    "                self.public_optimizer.optimizer.zero_grad()\n",
    "                p_output = self.public(z)\n",
    "                p_loss = self.cross_entloss(p_output, labels[public_label])\n",
    "                \n",
    "            # Loss =====================================\n",
    "            if privacy_flag:\n",
    "                d_output = self.discriminator(z)\n",
    "                d_prob = self.softmaxlayer(d_output)\n",
    "                \n",
    "                # Calculate for private task entropy/log liklihood\n",
    "                if arl_setting == 'maxent-arl':\n",
    "                    entropy_loss = -self.entropy_loss(d_prob)\n",
    "                elif arl_setting == 'ml-arl':\n",
    "                    entropy_loss = self.nll_loss(torch.log(d_prob+1e-16), labels[private_label])\n",
    "                else:\n",
    "                    raise Exception(\"Invalid ARL.\")\n",
    "                s_loss = -entropy_loss\n",
    "            \n",
    "            # Combined loss to update SSD (+ public task)\n",
    "            if privacy_flag:\n",
    "                if public_flag:\n",
    "#                     if original:\n",
    "                        loss = p_loss\n",
    "#                     else:\n",
    "#                         loss = (1-self.alpha)*(p_loss) + self.alpha*s_loss\n",
    "            else:\n",
    "                loss = p_loss\n",
    "            \n",
    "            if original and public_flag:\n",
    "                self.discriminator_optimizer.optimizer.zero_grad()\n",
    "                d_output = self.discriminator(z.detach())\n",
    "                d_loss = self.cross_entloss(d_output, labels[private_label]) \n",
    "                loss += d_loss \n",
    "\n",
    "            # Backprop ==================================\n",
    "            self.optimizer.optimizer.zero_grad()  \n",
    "            loss.backward()\n",
    "            if public_flag: _, _, _ = self.public_optimizer.step_all()                \n",
    "            enc_grad_norm, self.grad_norm, shrinkage = self.optimizer.step_all()\n",
    "            \n",
    "            if privacy_flag: \n",
    "                if original:\n",
    "                    _, _, _ = self.discriminator_optimizer.step_all()\n",
    "                else:\n",
    "                    # 2. Train Discriminator ====================\n",
    "                    self.discriminator_optimizer.optimizer.zero_grad()\n",
    "                    d_loss, _ = self.train_net(net = self.discriminator, \n",
    "                                               net_optim = self.discriminator_optimizer,\n",
    "                                               net_input = z.detach(), \n",
    "                                               net_labels = labels[private_label]\n",
    "                                              )    \n",
    "        \n",
    "            # 3. Save and Show Results ==================\n",
    "            self.running_loss += loss.item() / debug_steps\n",
    "            self.report_grad_norm += self.grad_norm / debug_steps\n",
    "            if privacy_flag: \n",
    "                self.running_discriminator_loss += d_loss.item() / debug_steps\n",
    "                self.running_discriminator_entropy += entropy_loss.item() /debug_steps\n",
    "                \n",
    "            if public_flag:\n",
    "                self.running_public_loss += p_loss.item() / debug_steps\n",
    "            \n",
    "            #  Show the average results\n",
    "            if i and i % debug_steps == 0:\n",
    "                self.log(f\"Epoch:{epoch:02d}, Step:{i}, \" +\n",
    "                             f\"Loss:{self.running_loss:.4f}, \" +\n",
    "                             f\"ARL Loss:{self.running_discriminator_entropy:.4f}, \" + \n",
    "                             f\"D Loss:{self.running_discriminator_loss:.4f}, \" + \n",
    "                             f\"P Loss:{self.running_public_loss:.4f}, \" +\n",
    "                             f\"Grad Norm:{self.report_grad_norm:.4f}\"\n",
    "                            )\n",
    "                self.meters_reset()              \n",
    "    \n",
    "    def test(self, test_loader, private_label, public_label, d_loc, privacy_flag = False, public_flag=False):\n",
    "        \n",
    "        self.net.eval()\n",
    "        if privacy_flag: self.discriminator.eval()\n",
    "        if public_flag: self.public.eval()\n",
    "        self.meters_reset()\n",
    "        num = 0\n",
    "        self.net.test = True\n",
    "\n",
    "        for i, (X,y) in enumerate(test_loader):\n",
    "\n",
    "            num += 1\n",
    "            if i%10 == 0: print(end='.')\n",
    "            labels = y[:,0].to(device), y[:,1].to(device), y[:,2].to(device)\n",
    "            X = X.to(device)\n",
    "\n",
    "            # Sensitive label\n",
    "            \n",
    "            #  Feedforward\n",
    "            with torch.no_grad():\n",
    "                z = self.net(X, d_loc)\n",
    "                \n",
    "                if privacy_flag:\n",
    "                    d_output = self.discriminator(z)\n",
    "                    d_prob = self.softmaxlayer(d_output)\n",
    "                    entropy_loss = -self.entropy_loss(d_prob)\n",
    "                    d_loss = self.nll_loss(torch.log(d_prob+1e-16), labels[private_label])\n",
    "                    \n",
    "                if public_flag:\n",
    "                    p_output = self.public(z)\n",
    "                    p_prob = self.softmaxlayer(p_output)\n",
    "                    p_entropy_loss = -self.entropy_loss(p_prob)\n",
    "                    p_loss = self.nll_loss(torch.log(p_prob+1e-16), labels[public_label])\n",
    "                    loss = p_loss\n",
    "\n",
    "            #  Save numerical results\n",
    "            self.running_loss += loss.item()\n",
    "            if privacy_flag:\n",
    "                self.running_discriminator_loss += d_loss.item()\n",
    "                self.running_discriminator_entropy += entropy_loss.item()\n",
    "            if public_flag:\n",
    "                self.running_public_loss += p_loss.item()\n",
    "\n",
    "        self.net.test = False\n",
    "\n",
    "        return (self.running_loss/num, self.running_discriminator_loss/num, self.running_discriminator_entropy/num, self.running_public_loss/num)\n",
    "    \n",
    "    \n",
    "    def arl_attack(self, train_loader, private_label, public_label, attacker_loc=None, privacy_flag=True, \n",
    "                   public_flag = False, debug_steps=40, epoch=-1, verbose=False):\n",
    "        ''' ===================================================================================\n",
    "        Main Algorithm:\n",
    "            1. Train encoder and decoder with discriminator loss\n",
    "            2. Train discriminator\n",
    "        ======================================================================================== '''\n",
    "        self.net.eval()\n",
    "        self.discriminator.train(True)\n",
    "        self.meters_reset()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "            # Prepare data\n",
    "            if i%10 == 0 and i: print(end='.')\n",
    "            labels = y[:,0].to(device), y[:,1].to(device), y[:,2].to(device)\n",
    "            X = X.to(device)\n",
    "\n",
    "            # 1. Train Encoder + Predictor =============\n",
    "            with torch.no_grad():\n",
    "                z = self.net(X, attacker_loc)\n",
    "                \n",
    "                if public_flag:\n",
    "                    p_output = self.public(z)\n",
    "                    p_prob = self.softmaxlayer(p_output)\n",
    "                    p_entropy_loss = -self.entropy_loss(p_prob)\n",
    "                    p_loss = self.cross_entloss(p_output, labels[public_label])\n",
    "                \n",
    "            if privacy_flag: \n",
    "                self.discriminator_optimizer.optimizer.zero_grad()\n",
    "                d_loss, grad_norm = self.train_net(net = self.discriminator, \n",
    "                                           net_optim = self.discriminator_optimizer,\n",
    "                                           net_input = z.detach(), \n",
    "                                           net_labels = labels[private_label])    \n",
    "\n",
    "            # 3. Save and Show Results ==================\n",
    "            self.running_loss += d_loss.item() / debug_steps\n",
    "            self.report_grad_norm += grad_norm / debug_steps\n",
    "            if public_flag:\n",
    "                self.running_public_loss += p_loss.item() / debug_steps\n",
    "                self.running_public_entropy += p_entropy_loss / debug_steps\n",
    "            \n",
    "            #  Show the average results\n",
    "            if i and i % debug_steps == 0:\n",
    "                self.log(f\"Epoch:{epoch:02d}, Step:{i:3d}, \" + \n",
    "                         f\"Attacker Loss:{self.running_loss:.4f}, \" +\n",
    "                         f\"P Loss:{self.running_public_loss:.4f}, \" +\n",
    "                         f\"P Ent: {self.running_public_entropy:.4f}, \" +\n",
    "                         f\"Grad Norm:{self.report_grad_norm:.4f}\"\n",
    "                         )\n",
    "                self.meters_reset()\n",
    "\n",
    "                # for param in net.classification_headers.parameters():\n",
    "                #     print(f\"Classification:{param[0][0]}\")\n",
    "                #     break                \n",
    "        \n",
    "    def test_attack(self, criterion, attacker_loc, privacy_flag = True):\n",
    "        \n",
    "        self.net.eval()\n",
    "        self.discriminator.eval()\n",
    "        self.meters_reset()\n",
    "        num = 0\n",
    "        self.net.test = True\n",
    "\n",
    "        for i, (X,y) in enumerate(test_loader):\n",
    "\n",
    "            num += 1\n",
    "            if i%10 == 0: print(end='.')\n",
    "            self.images.resize_(data[0].size()).copy_(data[0])\n",
    "            self.boxes.resize_(data[1].size()).copy_(data[1])\n",
    "            self.labels.resize_(data[2].size()).copy_(data[2])\n",
    "\n",
    "            # Sensitive label\n",
    "            sensitive_labels = data[3] if len(data) == 4 else self.online_find_labels(self.labels, self.sensitive_classes)\n",
    "            self.sensitive_labels.resize_(sensitive_labels.size()).copy_(sensitive_labels)   \n",
    "            \n",
    "            #  Feedforward\n",
    "            with torch.no_grad():\n",
    "                confidence, locations, z = self.net(self.images, attacker_loc)\n",
    "                d_output = self.discriminator(z)\n",
    "                d_prob = self.softmaxlayer(d_output)\n",
    "                d_loss = self.nll_loss(torch.log(d_prob+1e-16), self.sensitive_labels)\n",
    "                entropy_loss = -self.entropy_loss(d_prob)\n",
    "                s_loss = -entropy_loss \n",
    "\n",
    "                #  Loss\n",
    "                class_loss, reg_loss = criterion(confidence, locations, self.labels, self.boxes)\n",
    "                loss = class_loss + reg_loss\n",
    "\n",
    "            #  Save numerical results\n",
    "            self.running_loss += loss.item()\n",
    "            self.running_regression_loss += reg_loss.item()\n",
    "            self.running_classification_loss += class_loss.item()\n",
    "            self.running_discriminator_loss += d_loss.item()\n",
    "            self.running_discriminator_entropy += entropy_loss.item()\n",
    "\n",
    "                 \n",
    "\n",
    "        self.net.test = False\n",
    "\n",
    "        return self.running_loss/num, self.running_regression_loss/num, self.running_classification_loss/num, self.running_discriminator_loss/num, self.running_discriminator_entropy/num\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d62b20",
   "metadata": {},
   "source": [
    "# Encoder, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db330258",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Code for a Multi Label Neural Network\n",
    "    The Initial (Shared) layers are high-level feature extractors using convolutions.\n",
    "    The three lower level feature extractors consist of another convolutional layer followed by a few Dense layers.\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# High level feature extractor network (Adopted VGG type structure)\n",
    "class highLevelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(highLevelNN, self).__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            # first batch (32)\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # second batch (64)\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Third Batch (128)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, d_loc):\n",
    "        out = self.CNN(x)\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def save(self, privacy_flag, public_flag, alpha, max_grad_norm, epoch, optimizers, schedulers, val_loss, val_d_ent, val_d_loss, \n",
    "             sensitive_classes, model_path, discriminator_loc=None, discriminator=None, public=None, val_p_loss=None, public_classes=None,\n",
    "             arl_setting = None):\n",
    "        \n",
    "        dic = {'privacy_flag': privacy_flag, \n",
    "               'public_flag': public_flag,\n",
    "               'epoch': epoch,\n",
    "               'model_state_dict': self.state_dict(),\n",
    "               'max_grad_norm': max_grad_norm, \n",
    "               'optimizer_state_dict': optimizers[0].optimizer.state_dict(),\n",
    "               'scheduler_stat_dict': schedulers[0].state_dict() if schedulers[0] else None,\n",
    "               'loss': val_loss,\n",
    "              }\n",
    "        \n",
    "        if discriminator:\n",
    "            dic['alpha'] = alpha\n",
    "            dic['arl_setting'] = arl_setting\n",
    "            dic['sensitive_classes'] = sensitive_classes\n",
    "            dic['discriminator_loc'] = discriminator_loc\n",
    "            dic['entropy'] = val_d_ent\n",
    "            dic['discrimiantor_loss'] = val_d_loss\n",
    "            dic['discriminator_optimizer_state_dict'] = optimizers[1].optimizer.state_dict()\n",
    "            dic['discriminator_scheduler_state_dict'] = schedulers[1].state_dict() if schedulers[1] else None\n",
    "            dic['discriminator_state_dict'] = discriminator.state_dict()\n",
    "            \n",
    "        if public_flag:\n",
    "            dic['public_state_dict'] = public.state_dict()\n",
    "            dic['public_optimizer_state_dict'] = optimizers[2].optimizer.state_dict()\n",
    "            dic['public_scheduler_state_dict'] = schedulers[2].state_dict() if schedulers[2] else None\n",
    "            dic['public_loss'] = val_p_loss\n",
    "            dic['public_classes'] = public_classes\n",
    "            \n",
    "        torch.save(dic, model_path) \n",
    "        \n",
    "    def save_attack(self, privacy_flag, public_flag, alpha, max_grad_norm, loss, epoch, optimizers, schedulers, attacker_loss, public_classes,\n",
    "                    sensitive_classes, entropy, model_path, attacker_loc=None, public=None, attacker=None, name=None):\n",
    "        \n",
    "        dic = {'privacy_flag': privacy_flag, \n",
    "               'epoch': epoch,\n",
    "               'model_state_dict': self.state_dict(),\n",
    "               'max_grad_norm': max_grad_norm, \n",
    "               'loss': loss,\n",
    "               'public_flag': public_flag\n",
    "              }\n",
    "        if public_flag:\n",
    "            dic['public_classes'] = public_classes\n",
    "            dic['public_state_dict'] = public.state_dict()\n",
    "            dic['public_optimizer_state_dict'] = optimizers[2].optimizer.state_dict() if optimizers[2] else None\n",
    "            dic['public_scheduler_state_dict'] = schedulers[2].state_dict() if schedulers[2] else None\n",
    "        if privacy_flag:\n",
    "            dic['alpha'] = alpha\n",
    "            \n",
    "        dic['attacked_model'] = name\n",
    "        dic['sensitive_classes'] = sensitive_classes\n",
    "        dic['attacker_loss'] = attacker_loss\n",
    "        dic['attacker_loc'] = attacker_loc\n",
    "        dic['attacker_optimizer_state_dict'] = optimizers[1].optimizer.state_dict()\n",
    "        dic['attacker_scheduler_state_dict'] = schedulers[1].optimizer.state_dict() if schedulers[1] else None\n",
    "        dic['attacker_state_dict'] = attacker.state_dict()\n",
    "        dic['entropy'] = entropy\n",
    "            \n",
    "        torch.save(dic, model_path)\n",
    "\n",
    "# Low level feature extraction module\n",
    "class lowLevelNN(nn.Module):\n",
    "    def __init__(self, num_out):\n",
    "        super(lowLevelNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(in_features=2048, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=2, padding=1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=3, stride=2, padding=1))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TridentNN(nn.Module):\n",
    "    def __init__(self, num_age, num_gen, num_eth):\n",
    "        super(TridentNN, self).__init__()\n",
    "        # Construct the high level neural network\n",
    "        self.CNN = highLevelNN()\n",
    "        # Construct the low level neural networks\n",
    "        self.ageNN = lowLevelNN(num_out=num_age)\n",
    "        self.genNN = lowLevelNN(num_out=num_gen)\n",
    "        self.ethNN = lowLevelNN(num_out=num_eth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x, 0)\n",
    "        age = self.ageNN(x)\n",
    "        gen = self.genNN(x)\n",
    "        eth = self.ethNN(x)\n",
    "\n",
    "        return age, gen, eth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6cf837",
   "metadata": {},
   "source": [
    "## Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "584ed6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torch.nn.modules.loss import _Loss\n",
    "\n",
    "class EntropyLoss(_Loss):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(EntropyLoss, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    # input is probability distribution of output classes\n",
    "    def forward(self, input):\n",
    "        if (input < 0).any() or (input > 1).any():\n",
    "            raise Exception('Entropy Loss takes probabilities 0<=input<=1')\n",
    "\n",
    "        input = input + 1e-16  # for numerical stability while taking log\n",
    "        H = torch.mean(torch.sum(input * torch.log(input), dim=1))\n",
    "        \n",
    "        return H\n",
    "    \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadb988",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d77c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optim(object):\n",
    "\n",
    "    def _makeOptimizer(self):\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.lr, momentum=self.sgd_momentum, weight_decay=self.sgd_weight_decay)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.lr)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.lr)\n",
    "        elif self.method == 'adam':\n",
    "#             self.optimizer = optim.Adam(self.params, lr=self.lr)\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.lr, betas=(self.adam_momentum, 0.999))\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def __init__(self, params, method, enc_params = [0],  max_grad_norm = 5, lr = 0.001, lr_decay=1,\n",
    "                 start_decay_at= 60, adam_momentum=0.9, sgd_momentum = 0.9, sgd_weight_decay = 5e-4, privacy_flag=False):\n",
    "        \n",
    "        self.params = params  # careful: params may be a generator\n",
    "        self.encoder_params = list(enc_params)\n",
    "        self.last_ppl = None\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_at = start_decay_at\n",
    "        self.start_decay = False\n",
    "        self.adam_momentum = adam_momentum\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.sgd_weight_decay = sgd_weight_decay\n",
    "        self._makeOptimizer()\n",
    "\n",
    "    def step_all(self):\n",
    "        #  Calculate encoder gradient norm\n",
    "        \n",
    "        enc_grad_norm = 0.0\n",
    "        for param in self.encoder_params:\n",
    "            enc_grad_norm += math.pow(param.grad.data.norm(), 2) if param.grad != None else 0.0\n",
    "        enc_grad_norm = math.sqrt(enc_grad_norm)\n",
    "\n",
    "        #  Calculate all gradient norm\n",
    "        grad_norm = 0.0\n",
    "        for param in self.params:\n",
    "            for p in param['params']:\n",
    "                grad_norm += math.pow(p.grad.data.norm(), 2) \n",
    "        grad_norm = math.sqrt(grad_norm)\n",
    "\n",
    "        #  Shrink the graident if the gradients are too large\n",
    "        shrinkage = self.max_grad_norm / grad_norm\n",
    "        for param in self.params:\n",
    "            for p in param['params']:\n",
    "                if shrinkage < 1:\n",
    "                    p.grad.data.mul_(shrinkage)\n",
    "\n",
    "        #  Start updating\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return enc_grad_norm, grad_norm, shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e203867",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c8ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSetup:\n",
    "    def __init__(self, log=print, num_epochs=21, learning_rate = 1e-4, discriminator_lr = 1e-4, public_lr=1e-4, encoder_lr=1e-4):\n",
    "\n",
    "        self.log = log\n",
    "        \n",
    "        # network hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discriminator_lr = discriminator_lr\n",
    "        self.public_lr = public_lr\n",
    "        self.encoder_lr = encoder_lr\n",
    "        self.base_net_lr =  self.learning_rate\n",
    "        self.extra_layers_lr = self.learning_rate\n",
    "        self.num_classes = 21\n",
    "        self.scheduler = None\n",
    "        \n",
    "        # ARL setting\n",
    "        self.privacy_flag = True\n",
    "        self.public_flag = False\n",
    "        self.attack_flag = False\n",
    "        self.save = True\n",
    "        self.saving_more = True\n",
    "        self.continue_training = False\n",
    "\n",
    "        # encoder setting\n",
    "        self.public_headers = None\n",
    "        self.public_optimizer = None\n",
    "        self.public_scheduler = None\n",
    "        \n",
    "        self.discriminator_headers = None\n",
    "        self.discriminator_optimizer = None\n",
    "        self.discriminator_scheduler = None\n",
    "\n",
    "        # network general setting        \n",
    "        self.last_epoch = -1\n",
    "        self.num_epochs = num_epochs\n",
    "        self.loading_net = \"basenet\"\n",
    "        self.scheduler_type = 'cosine'\n",
    "\n",
    "        # optimizer setting\n",
    "        self.opt_type = 'sgd'\n",
    "        self.max_grad_norm = 6\n",
    "        self.verbose = False\n",
    "        self.t_max = 200\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.0005\n",
    "        self.min_loss = -10000.0\n",
    "        \n",
    "        self.timer = Timer()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            self.log(\"Use cuda:0\")\n",
    "    \n",
    "    def set_directory(self):\n",
    "        save_path, experiment_time = create_dir()\n",
    "        self.log(f\"=\"*45 + f\" Training Session {today}-{experiment_time} \" + f\"=\"*45)\n",
    "        \n",
    "        if not os.path.exists(save_path): os.mkdir(save_path)\n",
    "        self.checkpoint_folder = save_path\n",
    "        \n",
    "        if self.save: self.log(f\"Model will be stored at {self.checkpoint_folder}\")\n",
    "    \n",
    "    def build_discriminator(self, num_out):\n",
    "        return lowLevelNN(num_out=num_out)\n",
    "    \n",
    "    def build_network(self):\n",
    "        self.log(\"Build Network.\")\n",
    "        return highLevelNN()\n",
    "\n",
    "    def build_init(self, load_path=None):\n",
    "        timer = Timer()\n",
    "        timer.start(\"Load Model\")\n",
    "\n",
    "    def build_scheduler(self, optimizer, name = 'Network', privacy_flag = False):\n",
    "        \n",
    "        if self.scheduler_type == \"multi-step\":\n",
    "            self.log(\"Use MultiStepLR scheduler.\")\n",
    "            milestones = [int(v.strip()) for v in \"80,100\".split(\",\")]\n",
    "            return MultiStepLR(optimizer, milestones=milestones, gamma=0.1, last_epoch=last_epoch)\n",
    "        else:\n",
    "            self.log(name + f\" build [{self.scheduler_type}] CosineAnnealingLR scheduler. Max iter.={self.t_max}, last epoch={self.last_epoch}.\")\n",
    "            return CosineAnnealingLR(optimizer, self.t_max, last_epoch=self.last_epoch)\n",
    "\n",
    "    def build_optim(self, parameters=None, name='Network', learning_rate=1e-4, opt_type = 'sgd'):\n",
    "        self.log(name +  f\" build {opt_type} optimizer in Optim, learning_rate = {learning_rate}, momentum={self.momentum}, \" + \n",
    "                         f\"weight_decay={self.weight_decay}, \" + \n",
    "                         f\"max gradient norm={self.max_grad_norm}.\")   \n",
    " \n",
    "        return Optim(params = [{'params': parameters,'lr': learning_rate}], \n",
    "                     method = opt_type, \n",
    "                     enc_params = [],\n",
    "                     max_grad_norm = self.max_grad_norm, \n",
    "                     lr=learning_rate,\n",
    "                     sgd_momentum=self.momentum,\n",
    "                     sgd_weight_decay=self.weight_decay\n",
    "                     )\n",
    "    def build_rest(self):\n",
    "        if not self.save: self.log(f\"Not Saving model.\")\n",
    "            \n",
    "    # ==========================================================================\n",
    "    #                               Main Logic\n",
    "    # ==========================================================================\n",
    "    def build(self, load_path, privacy_flag, public_flag, prv_class_num, pub_class_num = 2, opt_type='adam', discriminator_opt_type='adam', is_attack=False):\n",
    "        \n",
    "        self.public_flag = public_flag\n",
    "        self.privacy_flag = privacy_flag\n",
    "        self.set_directory()\n",
    "        \n",
    "        # create Network [encoder and decoder]\n",
    "        self.net = self.build_network()\n",
    "        if not is_attack: self.optimizer = self.build_optim(parameters = self.net.parameters(),\n",
    "                                                            learning_rate = 0.001,\n",
    "                                                            opt_type=opt_type)\n",
    "        \n",
    "        # public task\n",
    "        if self.public_flag:\n",
    "            self.public_flag = public_flag\n",
    "            self.public_headers = self.build_discriminator(num_out = pub_class_num)\n",
    "            if not is_attack: self.public_optimizer = self.build_optim(name='Public',\n",
    "                                                                       parameters = self.public_headers.parameters(),\n",
    "                                                                       learning_rate = 0.001,\n",
    "                                                                       opt_type = discriminator_opt_type)\n",
    "            self.public_headers.to(device)\n",
    "        \n",
    "        # private task\n",
    "        if self.privacy_flag:   \n",
    "            build_name = 'Attacker' if is_attack else 'Discrimiantor'\n",
    "            self.discriminator_headers = self.build_discriminator(num_out = prv_class_num)\n",
    "            self.discriminator_optimizer = self.build_optim(name=build_name,\n",
    "                                                            learning_rate = 0.001,\n",
    "                                                            parameters = self.discriminator_headers.parameters(),\n",
    "                                                            opt_type=discriminator_opt_type)\n",
    "            self.discriminator_headers.to(device)\n",
    "            \n",
    "        # loading parameters\n",
    "        self.build_rest()\n",
    "        self.net.to(device)\n",
    "        \n",
    "        if is_attack:\n",
    "            self._load_checkpoint(load_path)\n",
    "        \n",
    "    # ==========================================================================\n",
    "    #                             Main Logic End\n",
    "    # ==========================================================================\n",
    "    \n",
    "    def _load_checkpoint(self, load_path):\n",
    "        self.log(f\"Load the model from: {load_path}\")\n",
    "        self.ckpt = torch.load(load_path, map_location=lambda storage, loc: storage)\n",
    "        self.net.load_state_dict(self.ckpt['model_state_dict'])\n",
    "        \n",
    "        self.privacy_flag = self.ckpt['privacy_flag']\n",
    "        self.public_flag = self.ckpt['public_flag']\n",
    "        self.max_grad_norm = self.ckpt['max_grad_norm']\n",
    "        self.log(f\"Privacy Flag={self.privacy_flag}, Public Flag={self.public_flag}, max_grad_norm={self.max_grad_norm}\")\n",
    "        \n",
    "        if self.ckpt['privacy_flag']:\n",
    "            self.sensitive_classes = self.ckpt['sensitive_classes']\n",
    "            self.discriminator_loc = self.ckpt['discriminator_loc']\n",
    "            self.alpha = self.ckpt['alpha']\n",
    "            self.arl_setting = self.ckpt['arl_setting']\n",
    "            self.log(f\"alpha={self.alpha[0]}, sensitive_classes={self.sensitive_classes}, discriminator_loc={self.discriminator_loc}\")\n",
    "\n",
    "        if self.ckpt['public_flag']:\n",
    "            self.public_classes = self.ckpt['public_classes']\n",
    "            self.public_headers.load_state_dict(self.ckpt['public_state_dict'])\n",
    "            \n",
    "    \n",
    "    def attack(self, train_loader, val_loader, attacker_loc, verbose=False, val_epoch=5, skip_train=False, previous_model=''): \n",
    "        ''' ====================================================================\n",
    "        ARL Attack. \n",
    "        The discriminator is used as the attacker.\n",
    "        \n",
    "        \n",
    "        ==================================================================== '''\n",
    "        choice = ['age', 'gender', 'ethnicity']\n",
    "        private_label = -1\n",
    "        public_label = -1\n",
    "        attack_model = previous_model\n",
    "        \n",
    "        self.log(f\"Attacker Location={attacker_loc}. Attacked Model={previous_model}\")\n",
    "        \n",
    "        if self.privacy_flag:\n",
    "            self.log(f\"Discriminator Location={self.discriminator_loc}, Alpha={self.alpha[0]}, Sensitive Classes={self.sensitive_classes}, ARL Setting={self.arl_setting}\")\n",
    "            private_label = choice.index(self.sensitive_classes)\n",
    "        if self.public_flag:\n",
    "            self.log(f\"Public Classes = {self.public_classes}\")\n",
    "            public_label = choice.index(self.public_classes)\n",
    "            \n",
    "        \n",
    "        self.log(f\"Start attacking the model from epoch {self.last_epoch + 1}.\") \n",
    "        \n",
    "        nets = [self.net, self.discriminator_headers, self.public_headers]\n",
    "        optimizers = [None, self.discriminator_optimizer, self.public_optimizer]\n",
    "        schedulers = [None, None, None]\n",
    "\n",
    "        train_net = Maxtrain_Network(nets, optimizers, self.alpha, self.sensitive_classes, log=self.log)\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.last_epoch + 1, self.num_epochs):\n",
    "                                     \n",
    "            if not skip_train:\n",
    "                train_net.arl_attack(train_loader, \n",
    "                                     private_label = private_label, \n",
    "                                     public_label = public_label,\n",
    "                                     privacy_flag = self.privacy_flag, \n",
    "                                     public_flag  = self.public_flag, \n",
    "                                     attacker_loc = attacker_loc, \n",
    "                                     debug_steps = 80,\n",
    "                                     verbose=verbose,\n",
    "                                     epoch=epoch)\n",
    "            \n",
    "            if epoch % val_epoch == 0 or epoch == self.num_epochs - 1 :                \n",
    "                val_loss, val_d_loss, val_d_ent, val_p_loss = train_net.test(val_loader,\n",
    "                                                                             private_label = private_label, \n",
    "                                                                             public_label = public_label,\n",
    "                                                                             d_loc=attacker_loc,\n",
    "                                                                             privacy_flag=self.privacy_flag,\n",
    "                                                                             public_flag  = self.public_flag,\n",
    "                                                                            )\n",
    "                self.log(f\"Epoch:{epoch:02d}, \" +\n",
    "                         f\"Public Loss:{val_loss:.4f}, \" +\n",
    "                         f\"Entropy:{val_d_ent:.4f}, \" +\n",
    "                         f\"Attacker Loss:{val_d_loss:.4f}, \"\n",
    "                         )          \n",
    "                #  Save\n",
    "                if self.save:\n",
    "                    model_name = f\"{'utk-NN'}-attack-Ep{epoch:03d}-FM{attacker_loc}-Loss-{val_loss:.4f}+{val_d_loss:.4f}-Ent-{val_d_ent:.4f}.pth\"\n",
    "                    model_path = os.path.join(self.checkpoint_folder, model_name)\n",
    "                    self.net.save_attack(privacy_flag=self.privacy_flag, \n",
    "                                         public_flag = self.public_flag,\n",
    "                                         alpha = self.alpha, \n",
    "                                         max_grad_norm = self.max_grad_norm, \n",
    "                                         epoch = epoch,\n",
    "                                         optimizers = optimizers,\n",
    "                                         schedulers = schedulers, \n",
    "                                         loss = val_loss, \n",
    "                                         public = self.public_headers,\n",
    "                                         sensitive_classes = self.sensitive_classes,\n",
    "                                         public_classes = self.public_classes,\n",
    "                                         entropy = val_d_ent,\n",
    "                                         attacker = self.discriminator_headers,\n",
    "                                         attacker_loss = val_d_loss, \n",
    "                                         attacker_loc = attacker_loc,\n",
    "                                         model_path = model_path,\n",
    "                                         name = attack_model\n",
    "                                        )\n",
    "                    self.log(f\"Save models at: ~{model_path[48:]}\")\n",
    "                    if os.path.exists(previous_model): os.remove(previous_model)\n",
    "                    previous_model = model_path\n",
    "    \n",
    "    \n",
    "    def train(self, train_loader, val_loader, d_loc = 14, public_classes = [7], sensitive_classes=[15], alpha=1.0, verbose=False, val_epoch=5, \n",
    "              skip_train=False, previous_model='', arl_setting=None):\n",
    "        ''' ======================================================================\n",
    "        ARL Training\n",
    "        \n",
    "        ========================================================================== '''  \n",
    "        choice = ['age', 'gender', 'ethnicity']\n",
    "        private_label = -1\n",
    "        public_label = -1\n",
    "        \n",
    "        if self.privacy_flag:\n",
    "            self.log(f\"Discriminator location={d_loc}, Alpha={alpha}, Sensitive classes={sensitive_classes}, Arl_setting={arl_setting}\")\n",
    "            private_label = choice.index(sensitive_classes)\n",
    "        if self.public_flag:\n",
    "            self.log(f\"Public classes={public_classes}\")\n",
    "            public_label = choice.index(public_classes)\n",
    "            \n",
    "        self.log(f\"Start training from epoch {self.last_epoch + 1}.\") \n",
    "        \n",
    "        nets = [self.net, self.discriminator_headers, self.public_headers]\n",
    "        optimizers = [self.optimizer, self.discriminator_optimizer, self.public_optimizer]\n",
    "        schedulers = [self.scheduler, self.discriminator_scheduler, self.public_scheduler]\n",
    "        \n",
    "        if self.privacy_flag:\n",
    "            alpha = torch.tensor([alpha*1.0], requires_grad=True)\n",
    "\n",
    "        train_net = Maxtrain_Network(nets, optimizers, alpha, sensitive_classes, public_classes, log=self.log)\n",
    "\n",
    "        for epoch in range(self.last_epoch + 1, self.num_epochs):       \n",
    "            \n",
    "            if not skip_train:\n",
    "                train_net.arl_train(train_loader,\n",
    "                                    private_label = private_label, \n",
    "                                    public_label = public_label,\n",
    "                                    privacy_flag = self.privacy_flag, \n",
    "                                    public_flag  = self.public_flag, \n",
    "                                    d_loc=d_loc,\n",
    "                                    debug_steps=100, \n",
    "                                    epoch=epoch, \n",
    "                                    verbose=verbose,\n",
    "                                    arl_setting = arl_setting,\n",
    "                                   )\n",
    "\n",
    "            # Evaluation\n",
    "            if epoch % val_epoch == 0 or epoch == self.num_epochs - 1 :                \n",
    "\n",
    "                val_loss, val_d_loss, val_d_ent, val_p_loss = train_net.test(val_loader,\n",
    "                                                                             private_label = private_label, \n",
    "                                                                             public_label = public_label,\n",
    "                                                                             d_loc=d_loc,\n",
    "                                                                             privacy_flag=self.privacy_flag,\n",
    "                                                                             public_flag  = self.public_flag,\n",
    "                                                                            )\n",
    "                self.log(f\"Epoch:{epoch:02d}, \" +\n",
    "                         f\"Validation Loss:{val_loss:.4f}, \" +\n",
    "                         f\"Entropy:{val_d_ent:.4f}, \" +\n",
    "                         f\"D Loss:{val_d_loss:.4f}, \" +\n",
    "                         f\"P Loss:{val_p_loss:.4f}\"\n",
    "                         ) \n",
    "                \n",
    "                if self.save:\n",
    "                    model_name = f\"{'utk-NN'}-Ep{epoch:03d}\"\n",
    "                    if self.privacy_flag:\n",
    "                        model_name += f\"-FM{d_loc}-A{alpha.data[0]:.1f}\"\n",
    "                    model_name += f\"-Loss-{val_loss:.4f}\"\n",
    "                    if self.privacy_flag:\n",
    "                        model_name += f\"+{val_d_ent:.4f}-DL-{val_d_loss:.4f}\"\n",
    "                    if self.public_flag:\n",
    "                        model_name += f\"-PL-{val_p_loss:.4f}\"\n",
    "                    model_name += f\".pth\"\n",
    "                        \n",
    "                        \n",
    "                    model_path = os.path.join(self.checkpoint_folder, model_name)\n",
    "                    \n",
    "                    self.net.save(privacy_flag = self.privacy_flag, \n",
    "                                  public_flag = self.public_flag,\n",
    "                                  alpha = alpha, \n",
    "                                  max_grad_norm = self.max_grad_norm, \n",
    "                                  epoch = epoch, \n",
    "                                  optimizers = optimizers, \n",
    "                                  schedulers = schedulers, \n",
    "                                  val_loss = val_loss, \n",
    "                                  val_d_ent = val_d_ent, \n",
    "                                  val_d_loss = val_d_loss,\n",
    "                                  sensitive_classes = sensitive_classes,\n",
    "                                  discriminator_loc = d_loc,\n",
    "                                  discriminator= self.discriminator_headers,\n",
    "                                  model_path = model_path,\n",
    "                                  public=self.public_headers,\n",
    "                                  val_p_loss = val_p_loss,\n",
    "                                  public_classes = public_classes,\n",
    "                                  arl_setting = arl_setting\n",
    "                                 )\n",
    "                    \n",
    "                    self.log(f\"Save models at: ~{model_path[48:]}\")\n",
    "                    if os.path.exists(previous_model): os.remove(previous_model)\n",
    "                    previous_model = model_path\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52378c4c",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1b8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def create_dir():\n",
    "    experiment_time = 1\n",
    "    testing_dir = f\"/home/hjchris/research/mitigating/chris/utk_ckpt/{today}/{today}-{experiment_time}\"\n",
    "    while (os.path.exists(testing_dir) and len([name for name in os.listdir(testing_dir) if os.path.isfile(os.path.join(testing_dir, name))]) != 0):\n",
    "        experiment_time += 1\n",
    "        testing_dir = f\"/home/hjchris/research/mitigating/chris/utk_ckpt/{today}/{today}-{experiment_time}\"\n",
    "    \n",
    "    return testing_dir, experiment_time\n",
    "\n",
    "\n",
    "def write_doc(name, mode, dictionary, method=None, message=None):\n",
    "    time = datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    with open(\"chris/utk_ckpt/document.txt\", 'a') as file:\n",
    "        file.write(f\"{time} - File={name},\\tTask={mode},\\t\")  \n",
    "        \n",
    "        for item in dictionary.items():\n",
    "                file.write(f\"{item[0]}={item[1]},\\t \")\n",
    "                if item[0] == 'privacy_flag' and item[1] == False: break\n",
    "                if item[0] == 'public_flag' and item[1] == False: break\n",
    "                \n",
    "        if message:\n",
    "                file.write(\"msg=(\" + str(message) + \")\\t\")\n",
    "        file.write(\"end\\n\")\n",
    "\n",
    "        \n",
    "def write_doc_end(name, mode):\n",
    "    time = datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    with open(\"chris/utk_ckpt/document.txt\", 'a') as file:\n",
    "        file.write(f\"{time} - File {name}   {mode} ends.\\n\")\n",
    "        \n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0.0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def find_ssd_file(checkpoint_folder):\n",
    "    \n",
    "    filenames=[]\n",
    "    for filename in os.listdir(checkpoint_folder):\n",
    "        if \"pth\" in filename:\n",
    "            filenames.append(filename)\n",
    "            \n",
    "    ssd_path = checkpoint_folder + filenames[0]\n",
    "    epoch = filenames[0].split(\"Ep\")[1][:3]\n",
    "    eval_path = checkpoint_folder + f\"evaluation_{epoch}/\"\n",
    "    \n",
    "    if not os.path.exists(eval_path):os.mkdir(eval_path)\n",
    "        \n",
    "    print(f\"ssd path: {ssd_path}\")\n",
    "    print(f\"eval path: {eval_path}\")\n",
    "    return ssd_path, eval_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b8401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02 10:18:59,948 - train - INFO - Train Loader:178(batch size:64), Test Laoder:23\n",
      "2022-01-02 10:19:00,017 - train - INFO - Shape of training X: torch.Size([64, 1, 48, 48])\n",
      "2022-01-02 10:19:00,018 - train - INFO - Shape of y: torch.Size([64, 3])\n",
      "2022-01-02 10:19:00,018 - train - INFO - [Attack] Train Loader:119, Test Laoder:60\n",
      "2022-01-02 10:19:00,046 - train - INFO - Shape of training X: torch.Size([64, 1, 48, 48])\n",
      "2022-01-02 10:19:00,047 - train - INFO - Shape of y: torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataFrame_path = '/home/hjchris/repo/utkface-estimation/data/age_gender.gz'\n",
    "train_loader, test_loader, train_loader_attack, test_loader_attack, class_nums = read_data(dataFrame_path, \n",
    "                                                                                           train_batch=64, \n",
    "                                                                                           attack=True, \n",
    "                                                                                           test_split = 0.2, \n",
    "                                                                                           attack_split = 0.4,\n",
    "                                                                                           log=train_log.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d1c50",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d469b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02 10:19:00,067 - train - INFO - Use cuda:0\n",
      "2022-01-02 10:19:00,210 - train - INFO - ============================================= Training Session 0102_22-1 =============================================\n",
      "2022-01-02 10:19:00,211 - train - INFO - Model will be stored at /home/hjchris/research/mitigating/chris/utk_ckpt/0102_22/0102_22-1\n",
      "2022-01-02 10:19:00,212 - train - INFO - Build Network.\n",
      "2022-01-02 10:19:00,216 - train - INFO - Network build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:19:00,230 - train - INFO - Public build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:19:05,424 - train - INFO - Discrimiantor build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:19:05,427 - train - INFO - Discriminator location=last, Alpha=0.5, Sensitive classes=ethnicity, Arl_setting=maxent-arl\n",
      "2022-01-02 10:19:05,428 - train - INFO - Public classes=gender\n",
      "2022-01-02 10:19:05,429 - train - INFO - Start training from epoch 0.\n",
      ".....2022-01-02 10:19:10,056 - train - INFO - Epoch:00, Step:100, Loss:2.1726, ARL Loss:1.4700, D Loss:1.4735, P Loss:2.1726, Grad Norm:0.0207\n",
      "......2022-01-02 10:19:14,043 - train - INFO - Epoch:00, Validation Loss:0.5918, Entropy:1.3773, D Loss:1.4306, P Loss:0.5918\n",
      "2022-01-02 10:19:14,156 - train - INFO - Save models at: ~/0102_22/0102_22-1/utk-NN-Ep000-FMlast-A0.5-Loss-0.5918+1.3773-DL-1.4306-PL-0.5918.pth\n",
      ".....2022-01-02 10:19:20,229 - train - INFO - Epoch:01, Step:100, Loss:1.9893, ARL Loss:1.4619, D Loss:1.4536, P Loss:1.9893, Grad Norm:0.9876\n",
      "........2022-01-02 10:19:30,957 - train - INFO - Epoch:02, Step:100, Loss:1.7538, ARL Loss:1.3625, D Loss:1.3492, P Loss:1.7538, Grad Norm:1.1451\n",
      "........2022-01-02 10:19:41,686 - train - INFO - Epoch:03, Step:100, Loss:1.6153, ARL Loss:1.2662, D Loss:1.2628, P Loss:1.6153, Grad Norm:1.2776\n",
      "........2022-01-02 10:19:52,409 - train - INFO - Epoch:04, Step:100, Loss:1.6116, ARL Loss:1.2810, D Loss:1.2748, P Loss:1.6116, Grad Norm:1.2870\n",
      "........2022-01-02 10:20:03,133 - train - INFO - Epoch:05, Step:100, Loss:1.5076, ARL Loss:1.2032, D Loss:1.2023, P Loss:1.5076, Grad Norm:1.2514\n",
      "......2022-01-02 10:20:08,119 - train - INFO - Epoch:05, Validation Loss:0.3072, Entropy:1.1541, D Loss:1.0904, P Loss:0.3072\n",
      "2022-01-02 10:20:08,217 - train - INFO - Save models at: ~/0102_22/0102_22-1/utk-NN-Ep005-FMlast-A0.5-Loss-0.3072+1.1541-DL-1.0904-PL-0.3072.pth\n",
      ".....2022-01-02 10:20:14,308 - train - INFO - Epoch:06, Step:100, Loss:1.4263, ARL Loss:1.1381, D Loss:1.1421, P Loss:1.4263, Grad Norm:1.2943\n",
      "........2022-01-02 10:20:25,023 - train - INFO - Epoch:07, Step:100, Loss:1.3987, ARL Loss:1.1310, D Loss:1.1157, P Loss:1.3987, Grad Norm:1.2412\n",
      "........2022-01-02 10:20:35,718 - train - INFO - Epoch:08, Step:100, Loss:1.3601, ARL Loss:1.1008, D Loss:1.1000, P Loss:1.3601, Grad Norm:1.4121\n",
      "........2022-01-02 10:20:46,434 - train - INFO - Epoch:09, Step:100, Loss:1.2970, ARL Loss:1.0615, D Loss:1.0556, P Loss:1.2970, Grad Norm:1.2975\n",
      "........2022-01-02 10:20:57,149 - train - INFO - Epoch:10, Step:100, Loss:1.2810, ARL Loss:1.0413, D Loss:1.0389, P Loss:1.2810, Grad Norm:1.2755\n",
      "......2022-01-02 10:21:02,116 - train - INFO - Epoch:10, Validation Loss:0.2689, Entropy:1.0861, D Loss:0.9936, P Loss:0.2689\n",
      "2022-01-02 10:21:02,214 - train - INFO - Save models at: ~/0102_22/0102_22-1/utk-NN-Ep010-FMlast-A0.5-Loss-0.2689+1.0861-DL-0.9936-PL-0.2689.pth\n",
      ".....2022-01-02 10:21:08,306 - train - INFO - Epoch:11, Step:100, Loss:1.2516, ARL Loss:1.0053, D Loss:1.0168, P Loss:1.2516, Grad Norm:1.2616\n",
      "........2022-01-02 10:21:19,020 - train - INFO - Epoch:12, Step:100, Loss:1.2698, ARL Loss:1.0410, D Loss:1.0422, P Loss:1.2698, Grad Norm:1.3192\n",
      "........2022-01-02 10:21:29,724 - train - INFO - Epoch:13, Step:100, Loss:1.2309, ARL Loss:1.0132, D Loss:1.0092, P Loss:1.2309, Grad Norm:1.2361\n",
      "........2022-01-02 10:21:40,439 - train - INFO - Epoch:14, Step:100, Loss:1.2532, ARL Loss:1.0303, D Loss:1.0347, P Loss:1.2532, Grad Norm:1.3556\n",
      "........2022-01-02 10:21:51,145 - train - INFO - Epoch:15, Step:100, Loss:1.2084, ARL Loss:1.0061, D Loss:1.0006, P Loss:1.2084, Grad Norm:1.3845\n",
      "......2022-01-02 10:21:56,130 - train - INFO - Epoch:15, Validation Loss:0.2718, Entropy:1.0148, D Loss:0.9904, P Loss:0.2718\n",
      "2022-01-02 10:21:56,228 - train - INFO - Save models at: ~/0102_22/0102_22-1/utk-NN-Ep015-FMlast-A0.5-Loss-0.2718+1.0148-DL-0.9904-PL-0.2718.pth\n",
      ".....2022-01-02 10:22:02,322 - train - INFO - Epoch:16, Step:100, Loss:1.1707, ARL Loss:0.9606, D Loss:0.9704, P Loss:1.1707, Grad Norm:1.3553\n",
      "........2022-01-02 10:22:13,036 - train - INFO - Epoch:17, Step:100, Loss:1.1711, ARL Loss:0.9780, D Loss:0.9780, P Loss:1.1711, Grad Norm:1.3694\n",
      "........2022-01-02 10:22:23,759 - train - INFO - Epoch:18, Step:100, Loss:1.1485, ARL Loss:0.9498, D Loss:0.9431, P Loss:1.1485, Grad Norm:1.3240\n",
      "........2022-01-02 10:22:34,486 - train - INFO - Epoch:19, Step:100, Loss:1.1381, ARL Loss:0.9670, D Loss:0.9522, P Loss:1.1381, Grad Norm:1.3736\n",
      "........2022-01-02 10:22:45,212 - train - INFO - Epoch:20, Step:100, Loss:1.1187, ARL Loss:0.9285, D Loss:0.9437, P Loss:1.1187, Grad Norm:1.4165\n",
      "......2022-01-02 10:22:50,198 - train - INFO - Epoch:20, Validation Loss:0.2719, Entropy:0.8900, D Loss:1.0015, P Loss:0.2719\n",
      "2022-01-02 10:22:50,296 - train - INFO - Save models at: ~/0102_22/0102_22-1/utk-NN-Ep020-FMlast-A0.5-Loss-0.2719+0.8900-DL-1.0015-PL-0.2719.pth\n",
      "2022-01-02 10:22:50,307 - train - INFO - Use cuda:0\n",
      "2022-01-02 10:22:50,464 - train - INFO - ============================================= Training Session 0102_22-2 =============================================\n",
      "2022-01-02 10:22:50,465 - train - INFO - Model will be stored at /home/hjchris/research/mitigating/chris/utk_ckpt/0102_22/0102_22-2\n",
      "2022-01-02 10:22:50,466 - train - INFO - Build Network.\n",
      "2022-01-02 10:22:50,471 - train - INFO - Network build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:22:50,491 - train - INFO - Public build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:22:50,514 - train - INFO - Discrimiantor build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:22:50,518 - train - INFO - Discriminator location=last, Alpha=0.5, Sensitive classes=ethnicity, Arl_setting=maxent-arl\n",
      "2022-01-02 10:22:50,519 - train - INFO - Public classes=gender\n",
      "2022-01-02 10:22:50,520 - train - INFO - Start training from epoch 0.\n",
      ".....2022-01-02 10:22:56,609 - train - INFO - Epoch:00, Step:100, Loss:2.1695, ARL Loss:1.4722, D Loss:1.4740, P Loss:2.1695, Grad Norm:0.0461\n",
      "......2022-01-02 10:23:01,596 - train - INFO - Epoch:00, Validation Loss:0.5232, Entropy:1.4347, D Loss:1.4203, P Loss:0.5232\n",
      "2022-01-02 10:23:01,695 - train - INFO - Save models at: ~/0102_22/0102_22-2/utk-NN-Ep000-FMlast-A0.5-Loss-0.5232+1.4347-DL-1.4203-PL-0.5232.pth\n",
      ".....2022-01-02 10:23:07,773 - train - INFO - Epoch:01, Step:100, Loss:1.9108, ARL Loss:1.4175, D Loss:1.4080, P Loss:1.9108, Grad Norm:1.0271\n",
      "........2022-01-02 10:23:18,555 - train - INFO - Epoch:02, Step:100, Loss:1.7134, ARL Loss:1.3114, D Loss:1.3120, P Loss:1.7134, Grad Norm:1.4413\n",
      "........2022-01-02 10:23:29,316 - train - INFO - Epoch:03, Step:100, Loss:1.6042, ARL Loss:1.2469, D Loss:1.2453, P Loss:1.6042, Grad Norm:1.3077\n",
      "........2022-01-02 10:23:40,091 - train - INFO - Epoch:04, Step:100, Loss:1.5124, ARL Loss:1.1847, D Loss:1.1822, P Loss:1.5124, Grad Norm:1.2329\n",
      "........2022-01-02 10:23:50,846 - train - INFO - Epoch:05, Step:100, Loss:1.4491, ARL Loss:1.1201, D Loss:1.1234, P Loss:1.4491, Grad Norm:1.3419\n",
      "......2022-01-02 10:23:55,874 - train - INFO - Epoch:05, Validation Loss:0.3446, Entropy:1.1281, D Loss:1.0537, P Loss:0.3446\n",
      "2022-01-02 10:23:55,970 - train - INFO - Save models at: ~/0102_22/0102_22-2/utk-NN-Ep005-FMlast-A0.5-Loss-0.3446+1.1281-DL-1.0537-PL-0.3446.pth\n",
      ".....2022-01-02 10:24:02,122 - train - INFO - Epoch:06, Step:100, Loss:1.4279, ARL Loss:1.1142, D Loss:1.1085, P Loss:1.4279, Grad Norm:1.3848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........2022-01-02 10:24:12,885 - train - INFO - Epoch:07, Step:100, Loss:1.3528, ARL Loss:1.0660, D Loss:1.0617, P Loss:1.3528, Grad Norm:1.3213\n",
      "........2022-01-02 10:24:23,647 - train - INFO - Epoch:08, Step:100, Loss:1.3562, ARL Loss:1.0609, D Loss:1.0645, P Loss:1.3562, Grad Norm:1.3988\n",
      "........2022-01-02 10:24:34,402 - train - INFO - Epoch:09, Step:100, Loss:1.2651, ARL Loss:0.9950, D Loss:0.9974, P Loss:1.2651, Grad Norm:1.2839\n",
      "........2022-01-02 10:24:45,136 - train - INFO - Epoch:10, Step:100, Loss:1.2502, ARL Loss:0.9828, D Loss:0.9860, P Loss:1.2502, Grad Norm:1.3816\n",
      "......2022-01-02 10:24:50,145 - train - INFO - Epoch:10, Validation Loss:0.2848, Entropy:1.0104, D Loss:0.9845, P Loss:0.2848\n",
      "2022-01-02 10:24:50,241 - train - INFO - Save models at: ~/0102_22/0102_22-2/utk-NN-Ep010-FMlast-A0.5-Loss-0.2848+1.0104-DL-0.9845-PL-0.2848.pth\n",
      ".....2022-01-02 10:24:56,385 - train - INFO - Epoch:11, Step:100, Loss:1.2251, ARL Loss:0.9823, D Loss:0.9728, P Loss:1.2251, Grad Norm:1.4556\n",
      "........2022-01-02 10:25:07,171 - train - INFO - Epoch:12, Step:100, Loss:1.1910, ARL Loss:0.9520, D Loss:0.9505, P Loss:1.1910, Grad Norm:1.3285\n",
      "........2022-01-02 10:25:17,957 - train - INFO - Epoch:13, Step:100, Loss:1.1586, ARL Loss:0.9207, D Loss:0.9246, P Loss:1.1586, Grad Norm:1.4155\n",
      "........2022-01-02 10:25:28,753 - train - INFO - Epoch:14, Step:100, Loss:1.1552, ARL Loss:0.9120, D Loss:0.9246, P Loss:1.1552, Grad Norm:1.3675\n",
      "........2022-01-02 10:25:39,517 - train - INFO - Epoch:15, Step:100, Loss:1.1533, ARL Loss:0.9276, D Loss:0.9260, P Loss:1.1533, Grad Norm:1.3501\n",
      "......2022-01-02 10:25:44,524 - train - INFO - Epoch:15, Validation Loss:0.2762, Entropy:0.8740, D Loss:0.8873, P Loss:0.2762\n",
      "2022-01-02 10:25:44,621 - train - INFO - Save models at: ~/0102_22/0102_22-2/utk-NN-Ep015-FMlast-A0.5-Loss-0.2762+0.8740-DL-0.8873-PL-0.2762.pth\n",
      ".....2022-01-02 10:25:50,765 - train - INFO - Epoch:16, Step:100, Loss:1.0740, ARL Loss:0.8679, D Loss:0.8661, P Loss:1.0740, Grad Norm:1.3486\n",
      "........2022-01-02 10:26:01,540 - train - INFO - Epoch:17, Step:100, Loss:1.1181, ARL Loss:0.8966, D Loss:0.9092, P Loss:1.1181, Grad Norm:1.3733\n",
      "........2022-01-02 10:26:12,324 - train - INFO - Epoch:18, Step:100, Loss:1.0754, ARL Loss:0.8611, D Loss:0.8676, P Loss:1.0754, Grad Norm:1.4240\n",
      "........2022-01-02 10:26:23,050 - train - INFO - Epoch:19, Step:100, Loss:1.0624, ARL Loss:0.8576, D Loss:0.8605, P Loss:1.0624, Grad Norm:1.3474\n",
      "........2022-01-02 10:26:33,824 - train - INFO - Epoch:20, Step:100, Loss:1.0016, ARL Loss:0.8190, D Loss:0.8199, P Loss:1.0016, Grad Norm:1.5035\n",
      "......2022-01-02 10:26:38,862 - train - INFO - Epoch:20, Validation Loss:0.3195, Entropy:0.8882, D Loss:0.9636, P Loss:0.3195\n",
      "2022-01-02 10:26:38,959 - train - INFO - Save models at: ~/0102_22/0102_22-2/utk-NN-Ep020-FMlast-A0.5-Loss-0.3195+0.8882-DL-0.9636-PL-0.3195.pth\n",
      "2022-01-02 10:26:38,969 - train - INFO - Use cuda:0\n",
      "2022-01-02 10:26:39,122 - train - INFO - ============================================= Training Session 0102_22-3 =============================================\n",
      "2022-01-02 10:26:39,123 - train - INFO - Model will be stored at /home/hjchris/research/mitigating/chris/utk_ckpt/0102_22/0102_22-3\n",
      "2022-01-02 10:26:39,124 - train - INFO - Build Network.\n",
      "2022-01-02 10:26:39,129 - train - INFO - Network build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:26:39,150 - train - INFO - Public build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:26:39,172 - train - INFO - Discrimiantor build adam optimizer in Optim, learning_rate = 0.001, momentum=0.9, weight_decay=0.0005, max gradient norm=6.\n",
      "2022-01-02 10:26:39,176 - train - INFO - Discriminator location=last, Alpha=0.5, Sensitive classes=ethnicity, Arl_setting=maxent-arl\n",
      "2022-01-02 10:26:39,176 - train - INFO - Public classes=gender\n",
      "2022-01-02 10:26:39,177 - train - INFO - Start training from epoch 0.\n",
      ".....2022-01-02 10:26:45,292 - train - INFO - Epoch:00, Step:100, Loss:2.1812, ARL Loss:1.4829, D Loss:1.4821, P Loss:2.1812, Grad Norm:0.0005\n",
      "......2022-01-02 10:26:50,301 - train - INFO - Epoch:00, Validation Loss:0.6301, Entropy:1.4372, D Loss:1.4326, P Loss:0.6301\n",
      "2022-01-02 10:26:50,400 - train - INFO - Save models at: ~/0102_22/0102_22-3/utk-NN-Ep000-FMlast-A0.5-Loss-0.6301+1.4372-DL-1.4326-PL-0.6301.pth\n",
      ".....2022-01-02 10:26:56,508 - train - INFO - Epoch:01, Step:100, Loss:2.0055, ARL Loss:1.4321, D Loss:1.4224, P Loss:2.0055, Grad Norm:0.7146\n",
      "........2022-01-02 10:27:07,332 - train - INFO - Epoch:02, Step:100, Loss:1.8071, ARL Loss:1.3607, D Loss:1.3638, P Loss:1.8071, Grad Norm:1.3454\n",
      "........2022-01-02 10:27:18,094 - train - INFO - Epoch:03, Step:100, Loss:1.6328, ARL Loss:1.2659, D Loss:1.2695, P Loss:1.6328, Grad Norm:1.5311\n",
      "........2022-01-02 10:27:28,910 - train - INFO - Epoch:04, Step:100, Loss:1.5801, ARL Loss:1.2551, D Loss:1.2471, P Loss:1.5801, Grad Norm:1.3403\n",
      "........2022-01-02 10:27:39,674 - train - INFO - Epoch:05, Step:100, Loss:1.5043, ARL Loss:1.1911, D Loss:1.1933, P Loss:1.5043, Grad Norm:1.4237\n",
      "......2022-01-02 10:27:44,691 - train - INFO - Epoch:05, Validation Loss:0.2968, Entropy:1.1781, D Loss:1.1333, P Loss:0.2968\n",
      "2022-01-02 10:27:44,789 - train - INFO - Save models at: ~/0102_22/0102_22-3/utk-NN-Ep005-FMlast-A0.5-Loss-0.2968+1.1781-DL-1.1333-PL-0.2968.pth\n",
      ".....2022-01-02 10:27:50,880 - train - INFO - Epoch:06, Step:100, Loss:1.4510, ARL Loss:1.1517, D Loss:1.1541, P Loss:1.4510, Grad Norm:1.4280\n",
      "........2022-01-02 10:28:01,646 - train - INFO - Epoch:07, Step:100, Loss:1.3974, ARL Loss:1.1095, D Loss:1.1098, P Loss:1.3974, Grad Norm:1.3984\n",
      "........2022-01-02 10:28:12,421 - train - INFO - Epoch:08, Step:100, Loss:1.3497, ARL Loss:1.0776, D Loss:1.0709, P Loss:1.3497, Grad Norm:1.4088\n",
      "........2022-01-02 10:28:23,216 - train - INFO - Epoch:09, Step:100, Loss:1.3263, ARL Loss:1.0742, D Loss:1.0752, P Loss:1.3263, Grad Norm:1.4220\n",
      "........2022-01-02 10:28:34,022 - train - INFO - Epoch:10, Step:100, Loss:1.3055, ARL Loss:1.0492, D Loss:1.0514, P Loss:1.3055, Grad Norm:1.5613\n",
      "......2022-01-02 10:28:39,059 - train - INFO - Epoch:10, Validation Loss:0.2806, Entropy:0.9537, D Loss:1.0153, P Loss:0.2806\n",
      "2022-01-02 10:28:39,157 - train - INFO - Save models at: ~/0102_22/0102_22-3/utk-NN-Ep010-FMlast-A0.5-Loss-0.2806+0.9537-DL-1.0153-PL-0.2806.pth\n",
      ".....2022-01-02 10:28:45,287 - train - INFO - Epoch:11, Step:100, Loss:1.2779, ARL Loss:1.0182, D Loss:1.0224, P Loss:1.2779, Grad Norm:1.4750\n",
      "........2022-01-02 10:28:56,047 - train - INFO - Epoch:12, Step:100, Loss:1.2322, ARL Loss:0.9990, D Loss:0.9984, P Loss:1.2322, Grad Norm:1.4680\n",
      "........2022-01-02 10:29:06,830 - train - INFO - Epoch:13, Step:100, Loss:1.2187, ARL Loss:0.9904, D Loss:0.9919, P Loss:1.2187, Grad Norm:1.4978\n",
      "........2022-01-02 10:29:17,586 - train - INFO - Epoch:14, Step:100, Loss:1.2388, ARL Loss:1.0104, D Loss:1.0056, P Loss:1.2388, Grad Norm:1.5636\n",
      "........2022-01-02 10:29:28,310 - train - INFO - Epoch:15, Step:100, Loss:1.1982, ARL Loss:0.9728, D Loss:0.9803, P Loss:1.1982, Grad Norm:1.5734\n",
      "......2022-01-02 10:29:33,269 - train - INFO - Epoch:15, Validation Loss:0.2530, Entropy:0.9812, D Loss:0.9478, P Loss:0.2530\n",
      "2022-01-02 10:29:33,367 - train - INFO - Save models at: ~/0102_22/0102_22-3/utk-NN-Ep015-FMlast-A0.5-Loss-0.2530+0.9812-DL-0.9478-PL-0.2530.pth\n",
      ".....2022-01-02 10:29:39,470 - train - INFO - Epoch:16, Step:100, Loss:1.1632, ARL Loss:0.9433, D Loss:0.9512, P Loss:1.1632, Grad Norm:1.5107\n",
      "........2022-01-02 10:29:50,207 - train - INFO - Epoch:17, Step:100, Loss:1.1371, ARL Loss:0.9257, D Loss:0.9231, P Loss:1.1371, Grad Norm:1.5740\n",
      "........2022-01-02 10:30:00,941 - train - INFO - Epoch:18, Step:100, Loss:1.1346, ARL Loss:0.9374, D Loss:0.9320, P Loss:1.1346, Grad Norm:1.6288\n",
      "........2022-01-02 10:30:11,674 - train - INFO - Epoch:19, Step:100, Loss:1.1189, ARL Loss:0.9319, D Loss:0.9323, P Loss:1.1189, Grad Norm:1.5956\n",
      "........2022-01-02 10:30:22,408 - train - INFO - Epoch:20, Step:100, Loss:1.1077, ARL Loss:0.9097, D Loss:0.9082, P Loss:1.1077, Grad Norm:1.5333\n",
      "......2022-01-02 10:30:27,376 - train - INFO - Epoch:20, Validation Loss:0.2953, Entropy:0.8861, D Loss:0.9467, P Loss:0.2953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02 10:30:27,474 - train - INFO - Save models at: ~/0102_22/0102_22-3/utk-NN-Ep020-FMlast-A0.5-Loss-0.2953+0.8861-DL-0.9467-PL-0.2953.pth\n"
     ]
    }
   ],
   "source": [
    "message = None\n",
    "private_flag_set = [True]\n",
    "# arl_set = ['ml-arl']\n",
    "arl_set = ['maxent-arl']\n",
    "# arl_set = ['maxent-arl', 'ml-arl']\n",
    "\n",
    "private_label = 'ethnicity'\n",
    "public_label = 'gender'\n",
    "\n",
    "attack_dates = [\"1222\"]\n",
    "attack_nums = [9]\n",
    "attack_locs = [\"last\"]\n",
    "alphas = [0.5]\n",
    "\n",
    "todays_task_is_train = True\n",
    "\n",
    "if todays_task_is_train:\n",
    "    for pv in private_flag_set:\n",
    "        for arl_setting in arl_set:\n",
    "            for alpha in alphas:\n",
    "                train_dict ={'num_epochs': 21,\n",
    "                             'test_split': 0.2, \n",
    "                             'attack_split': 0.4, \n",
    "                             'public_flag':True,\n",
    "                             'public_label': public_label,\n",
    "                             'pub_class_num': class_nums[f'{public_label}_num'],\n",
    "                             'p_type': \"A\",\n",
    "                             'privacy_flag':pv,\n",
    "                             'method': arl_setting,\n",
    "                             'alpha': alpha,\n",
    "                             'private_label': private_label,\n",
    "                             'prv_class_num': class_nums[f'{private_label}_num'],\n",
    "                             'd_loc': 'last',\n",
    "                             'd_type': \"A\",\n",
    "                            }\n",
    "\n",
    "                \n",
    "                for rn in range(3):\n",
    "                    arl_model = TrainingSetup(log=train_log.info,\n",
    "                                              num_epochs=train_dict['num_epochs']\n",
    "                                             )\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    arl_model.build(load_path = 'a',\n",
    "                                    prv_class_num = train_dict['prv_class_num'],\n",
    "                                    pub_class_num = train_dict['pub_class_num'],\n",
    "                                    public_flag = train_dict['public_flag'],\n",
    "                                    privacy_flag = train_dict['privacy_flag']\n",
    "                                    )\n",
    "\n",
    "                    _, experiment_time = create_dir()\n",
    "                    write_doc(f\"{today}-{experiment_time}\", mode=\"train\", dictionary=train_dict, method=arl_setting, message=message)\n",
    "\n",
    "                    arl_model.train(train_loader, \n",
    "                                    test_loader,\n",
    "                                    arl_setting = arl_setting,\n",
    "                                    alpha = train_dict['alpha'],\n",
    "                                    d_loc = train_dict['d_loc'],\n",
    "                                    verbose = True, \n",
    "                                    public_classes = public_label,\n",
    "                                    sensitive_classes = private_label, \n",
    "                                    val_epoch = 5,\n",
    "                                    skip_train = False\n",
    "                                    )\n",
    "\n",
    "                    write_doc_end(f\"{today}-{experiment_time}\", mode=\"train\")\n",
    "\n",
    "\n",
    "else:\n",
    "    for att in range(min(len(attack_dates), len(attack_nums))):\n",
    "\n",
    "        attack_date = attack_dates[att]\n",
    "        attack_num = attack_nums[att]\n",
    "        attack_loc = attack_locs[att]\n",
    "        \n",
    "        attack_root = '/home/hjchris/research/mitigating/chris/utk_ckpt/'\n",
    "        attack_path, _ = find_ssd_file(attack_root + f\"{attack_date}_21/{attack_date}_21-{attack_num}/\")\n",
    "\n",
    "        attack_dict ={'num_epochs': 101,\n",
    "                      'attack_model':f\"{attack_date}_21-{attack_num}\", \n",
    "                      'attacker_loc': attack_loc,\n",
    "                      'a_type': \"A\",\n",
    "                      'public_label': public_label,\n",
    "                      'pub_class_num': class_nums[f'{public_label}_num'],\n",
    "                      'private_label': private_label,\n",
    "                      'prv_class_num': class_nums[f'{private_label}_num'],\n",
    "                     }\n",
    "\n",
    "        arl_model = TrainingSetup(log=train_log.info,\n",
    "                                  num_epochs=attack_dict['num_epochs']\n",
    "                                             )\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        arl_model.build(attack_path, \n",
    "                        privacy_flag = True, \n",
    "                        public_flag = True, \n",
    "                        prv_class_num = attack_dict['prv_class_num'],\n",
    "                        pub_class_num = attack_dict['pub_class_num'],\n",
    "                        is_attack = True\n",
    "                       )\n",
    "\n",
    "        _, experiment_time = create_dir()\n",
    "        write_doc(f\"{today}-{experiment_time}\", mode=\"attack\", dictionary=attack_dict)\n",
    "\n",
    "        arl_model.attack(train_loader_attack, \n",
    "                         test_loader_attack,\n",
    "                         attacker_loc = attack_dict['attacker_loc'],\n",
    "                         verbose=True, \n",
    "                         val_epoch=5,\n",
    "                         previous_model = attack_dict['attack_model'],\n",
    "                         skip_train=False\n",
    "                       )\n",
    "        \n",
    "        write_doc_end(f\"{today}-{experiment_time}\", mode=\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b375d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9cf585",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ed741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, net_path, class_nums, log=print):\n",
    "        \n",
    "        self.log = log\n",
    "        self.is_attack = True if \"attack\" in net_path else False\n",
    "        \n",
    "        self._load_checkpoint(net_path)\n",
    "        self._load(class_nums)\n",
    "    \n",
    "    def _load(self, class_nums):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.net = highLevelNN()\n",
    "        self.net.load_state_dict(self.ckpt['model_state_dict'])\n",
    "        \n",
    "        if self.privacy_flag:\n",
    "            self.discriminator = lowLevelNN(num_out=class_nums[f'{self.sensitive_classes}_num'])\n",
    "            if self.is_attack:\n",
    "                self.discriminator.load_state_dict(self.ckpt['attacker_state_dict'])\n",
    "            else:\n",
    "                self.discriminator.load_state_dict(self.ckpt['discriminator_state_dict'])\n",
    "            self.discriminator.to(device)\n",
    "            \n",
    "        if self.public_flag:\n",
    "            self.public = lowLevelNN(num_out=class_nums[f'{self.public_classes}_num'])\n",
    "            self.public.load_state_dict(self.ckpt['public_state_dict'])\n",
    "            self.public.to(device)\n",
    "        \n",
    "        self.net.to(device)\n",
    "        \n",
    "    def _load_checkpoint(self, net_path):\n",
    "        \n",
    "        print(\"Load checkpoint\")\n",
    "        self.ckpt = torch.load(net_path, map_location=lambda storage, loc: storage)\n",
    "        self.epoch = self.ckpt['epoch']\n",
    "        self.max_grad_norm = self.ckpt['max_grad_norm']\n",
    "        self.loss = self.ckpt['loss']\n",
    "        self.privacy_flag = self.ckpt['privacy_flag']\n",
    "        self.public_flag = self.ckpt['public_flag']\n",
    "        self.discriminator_loc = -10\n",
    "        \n",
    "        self.log(f\"Epoch={self.epoch}, max_grad norm={self.max_grad_norm}, Loss={self.ckpt['loss']:.4f}\")\n",
    "        \n",
    "        if self.privacy_flag:\n",
    "            \n",
    "            self.discriminator_loc = self.ckpt['discriminator_loc'] if not \"attack\" in ssd_path else self.ckpt['attacker_loc']\n",
    "            self.sensitive_classes = self.ckpt['sensitive_classes']\n",
    "            self.arl_setting = self.ckpt['arl_setting'] if not \"attack\" in ssd_path else \"attack\"\n",
    "        \n",
    "            total_loss = self.ckpt['total_loss'] if 'tota_loss' in self.ckpt.keys() else self.ckpt['loss']\n",
    "            self.log(f\"alpha={self.ckpt['alpha'][0]}, Sensitive Classes={self.ckpt['sensitive_classes']}, Total Loss={total_loss:.4f}, \"+ \n",
    "                    f\"method={self.arl_setting}\")\n",
    "        \n",
    "        if self.public_flag:\n",
    "            self.public_classes = self.ckpt['public_classes']\n",
    "            self.log(f\"Public classes={self.public_classes}\") \n",
    "            \n",
    "            \n",
    "    def test(self, test_loader):\n",
    "        \n",
    "        choice = ['age', 'gender', 'ethnicity']\n",
    "        private_count = 0\n",
    "        public_count = 0\n",
    "        total = 0\n",
    "        self.net.eval()\n",
    "        \n",
    "        if self.privacy_flag: \n",
    "            self.discriminator.eval()\n",
    "            private_label = choice.index(self.sensitive_classes)\n",
    "        if self.public_flag: \n",
    "            self.public.eval()\n",
    "            public_label = choice.index(self.public_classes)\n",
    "\n",
    "        size = len(test_loader)\n",
    "        \n",
    "        for i, (X,y) in enumerate(test_loader):\n",
    "\n",
    "            if i%10 == 0: print(end='.')\n",
    "            labels = y[:,0].to(device), y[:,1].to(device), y[:,2].to(device)\n",
    "            X = X.to(device)\n",
    "            \n",
    "            #  Feedforward\n",
    "            with torch.no_grad():\n",
    "                z = self.net(X, self.discriminator_loc)\n",
    "                \n",
    "                if self.privacy_flag:\n",
    "                    d_output = self.discriminator(z)\n",
    "                    private_count += (d_output.argmax(1) == labels[private_label]).type(torch.float).sum().item()\n",
    "                    \n",
    "                if self.public_flag:\n",
    "                    p_output = self.public(z)\n",
    "                    public_count += (p_output.argmax(1) == labels[public_label]).type(torch.float).sum().item()\n",
    "                    \n",
    "            total += len(y)\n",
    "        \n",
    "        if self.privacy_flag:\n",
    "            self.log(f\"Private Accuracy [{self.sensitive_classes}] : {private_count/total:.4f}\")\n",
    "        if self.public_flag:\n",
    "            self.log(f\"Public Accuracy [{self.public_classes}] : {public_count/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfaea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Handler(log, log_dir, path_name):\n",
    "    file_item=1\n",
    "    logging_filename = f\"{path_name} ({today}-{file_item}).log\"\n",
    "\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "        print(f\"Created directory: {log_dir}\")\n",
    "\n",
    "    while os.path.exists(os.path.join(log_dir, logging_filename)):\n",
    "        file_item += 1\n",
    "        logging_filename = f\"{path_name} ({today}-{file_item}).log\"\n",
    "\n",
    "    print(f\"Log File = {log_dir}/{logging_filename}\")\n",
    "\n",
    "    if log.hasHandlers(): \n",
    "        log.removeHandler(inf_fileHandler)\n",
    "        log.removeHandler(streamHandler)\n",
    "    \n",
    "    fileHandler = logging.FileHandler(\"{0}/{1}\".format(log_dir, logging_filename))\n",
    "    fileHandler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\" ))\n",
    "    \n",
    "    log.addHandler(fileHandler)\n",
    "    log.addHandler(streamHandler)\n",
    "    \n",
    "    return fileHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2045fb",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_date = 1231\n",
    "exp_nums = [5]\n",
    "to_log = True\n",
    "\n",
    "for exp_num in exp_nums:\n",
    "    ssd_path, eval_path = find_ssd_file('/home/hjchris/research/mitigating/chris/utk_ckpt/' + f\"{inference_date}_21/{str(inference_date)}_21-{str(exp_num)}/\")\n",
    "\n",
    "    if to_log: \n",
    "        inf_fileHandler = create_Handler(inf_log, log_dir, ssd_path.split('/')[-2])\n",
    "    \n",
    "    arl_inference = Inference(ssd_path, class_nums, log=inf_log.info)\n",
    "    arl_inference.test(test_loader_attack)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f5b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e36f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2311cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tridentNN = TridentNN(class_nums['age_num'], class_nums['gender_num'], class_nums['ethnicity_num'])\n",
    "opt = torch.optim.Adam(tridentNN.parameters(), lr=0.001)\n",
    "\n",
    "train(train_loader, tridentNN, opt, num_epoch=2, print_every=20, log=print)\n",
    "print('Finished training, running the testing script...')\n",
    "test(test_loader, tridentNN, log=print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "215px",
    "width": "310px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.719px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
